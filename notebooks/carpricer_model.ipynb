{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d916ec",
   "metadata": {},
   "source": [
    "# Training and exploring the model evaluation\n",
    "\n",
    "Jupyter notebooks are great for work's orchestration and interactive exploration. However, authoring code in notebooks can quickly become a proactice that is hard to scale. Testing and source control may become challenging.\n",
    "\n",
    "In this notebook, we will explora how we can combine the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8300be",
   "metadata": {},
   "source": [
    "## Importing our library\n",
    "\n",
    "Core functionality for our model is implemented in Python libraries, which allows us to use more robust testing.\n",
    "\n",
    "The way we designed the library is structure as follows:\n",
    "\n",
    "* `carpricer.dataprep` contains logic related to data preparation, including transformations.\n",
    "* `carpricer.train` contains logic related to training and evaluating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d4ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import carpricer.dataprep as dataprep\n",
    "from carpricer.dataprep import transformations\n",
    "from carpricer.train import evaluator, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abe68b",
   "metadata": {},
   "source": [
    "### Model's configuration and hyperparameters\n",
    "Our model's configuration is not stored in code, but in a configuration file. The choice we used here is on a `YAML` file. This makes easier to segregate configuration from code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864e125",
   "metadata": {},
   "source": [
    "Let's have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c638a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\r\n",
      "  baseline: \r\n",
      "    objective: reg:squarederror\r\n",
      "    min_child_weight: 4\r\n",
      "    colsample_bytree: .7\r\n",
      "    n_estimators: 200\r\n",
      "  tune:\r\n",
      "    cv: 5\r\n",
      "    objective: neg_mean_squared_error\r\n",
      "    search:\r\n",
      "      learning_rate:\r\n",
      "      - .03\r\n",
      "      - .05\r\n",
      "      - .07\r\n",
      "      - .5\r\n",
      "      max_depth:\r\n",
      "      - 3\r\n",
      "      - 7\r\n",
      "      - 9\r\n",
      "      - 20\r\n",
      "      alpha:\r\n",
      "      - 0\r\n",
      "      - .5\r\n",
      "      - 1\r\n",
      "      subsample:\r\n",
      "      - .6\r\n",
      "      - .8\r\n",
      "      - 1\r\n",
      "data:\r\n",
      "  label: lnprice\r\n",
      "  test_size: 0.3\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../src/carpricer.params.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a914ce",
   "metadata": {},
   "source": [
    "### Arguments to use\n",
    "\n",
    "Let's use the following arguments for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ac0f79",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "train_path = \"../data/car-prices/sample/automobile_prepared.csv\"\n",
    "params_file = \"../src/carpricer.params.yml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19f08e",
   "metadata": {},
   "source": [
    "The library `jobtools` has functionalities to quickly parse a `YAML` file to a namespace object that can use the \"dot notation\" to access it's members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a750bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobtools.arguments import file2namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afa4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = file2namespace(params_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e95f3b",
   "metadata": {},
   "source": [
    "## Loading data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce830e5",
   "metadata": {},
   "source": [
    "Let's use our preprocessing library to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2307d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = dataprep.read_and_split(train_path, params.data.test_size, params.data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ef72e",
   "metadata": {},
   "source": [
    "The method `scale_and_encode` was designed to preprocess a given dataset and also return the transformations fitted to it. This allows us to then store this transformations for when the model is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984f4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, transforms = transformations.scale_and_encode(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ac42f",
   "metadata": {},
   "source": [
    "Let's apply the same transformations to our testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d50f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = transforms.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22e3d1",
   "metadata": {},
   "source": [
    "## Training routine\n",
    "\n",
    "We will use Mlflow for tracking metrics, parameters and models in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a98ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d816c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f67c62",
   "metadata": {},
   "source": [
    "### The base model\n",
    "\n",
    "Our base model will be an XGBoost regressor with some basic configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c970ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df489d4c",
   "metadata": {},
   "source": [
    "We have some specific parameters for this regressor that are stored in the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c45d8cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(objective='reg:squarederror',\n",
       "          min_child_weight=4,\n",
       "          colsample_bytree=0.7,\n",
       "          n_estimators=200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.model.baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86ec86",
   "metadata": {},
   "source": [
    "We are going to pass this arguemnts to the constructor or the regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d6c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = XGBRegressor(silent=True, nthread=4, **params.model.baseline.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f24555",
   "metadata": {},
   "source": [
    "As a good practice, let's log the parameters used to our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_params(params.model.baseline.to_dict()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c14d44",
   "metadata": {},
   "source": [
    "### Searching for hyperparameters\n",
    "\n",
    "We designed a method to search for hyperparameters `fit_and_optimize`. This method recieves a training dataset, a base model and a grid with all the parameters to look for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cdb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = trainer.fit_and_optimize(X_train_transformed, \n",
    "                                  y_train, \n",
    "                                  base_model=base_model,\n",
    "                                  param_grid=params.model.tune.search.to_dict(),\n",
    "                                  cv=params.model.tune.cv,\n",
    "                                  scoring_fit=params.model.tune.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630cc69",
   "metadata": {},
   "source": [
    "### Evaluating the results\n",
    "\n",
    "Searching for the best configuration for the model may require some exploration. We are using the library `sklearn_evaluation` to evaluate results.\n",
    "\n",
    "`sklearn_evaluation` is used in the `evaluate_search` method where we pass the search results of the tunning and the parameters we are looking to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_search(search, plot_params_name=['learning_rate', 'max_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6893b",
   "metadata": {},
   "source": [
    "Let's fin the best model, the best parameters combination and log them with Mlflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = search.best_estimator_\n",
    "best_params = search.best_params_\n",
    "mlflow.log_params(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e451c9",
   "metadata": {},
   "source": [
    "We are going to calculate a couple of metrics for the chosen model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b7a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluator.evaluate_regressor(best_model, X_test_transformed, y_test)\n",
    "mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fbcb4f",
   "metadata": {},
   "source": [
    "### Constructing the inference pipeline\n",
    "\n",
    "Now we have a good model at hand, we can construct a pipeline with both the preprocessing step and an estimator. This pipleline can then be stored for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[('preprocessing', transforms),\n",
    "                                 ('model', best_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d0be0",
   "metadata": {},
   "source": [
    "#### Logging the model with Mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110bd03",
   "metadata": {},
   "source": [
    "We are going to log the winning model with mlflow. First let's see which is the expected inputs and outputs of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbacdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d22f2",
   "metadata": {},
   "source": [
    "Logging the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13987c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.log_model(\"model\", model_pipeline, signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c13691",
   "metadata": {},
   "source": [
    "Let's finish the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (carpricer)",
   "language": "python",
   "name": "carpricer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
