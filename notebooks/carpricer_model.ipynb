{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d916ec",
   "metadata": {},
   "source": [
    "# Training and exploring the model evaluation\n",
    "\n",
    "Jupyter notebooks are great for orchestrating work and for interactive exploration. However, authoring code in notebooks can quickly become hard to scale and error prone. Testing and source controling solutions built on notebooks is not trivial.\n",
    "\n",
    "In this notebook, we will explor how we can combine the practice of writing notebooks with the practice of building a Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8300be",
   "metadata": {},
   "source": [
    "## Importing our library\n",
    "\n",
    "Core functionality for our model is implemented in Python libraries, which allows us to use more robust testing. This project has a `pth` file configured to point to the `src` folder. Hence, any package that we created in such folder will be available for importing in the notebook.\n",
    "\n",
    "The `carpricer` library is included in the `src` folder. The way we designed the library is structure as follows:\n",
    "\n",
    "* `carpricer.dataprep` contains logic related to data preparation, including transformations.\n",
    "* `carpricer.train` contains logic related to training and evaluating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d4ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import carpricer.dataprep as dataprep\n",
    "from carpricer.dataprep import transformations\n",
    "from carpricer.train import evaluator, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abe68b",
   "metadata": {},
   "source": [
    "### Model's configuration and hyperparameters\n",
    "Our model's configuration is not stored in code, but in a configuration file. The choice we used here is on a `YAML` file. This makes easier to segregate configuration from code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864e125",
   "metadata": {},
   "source": [
    "Let's have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c638a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  baseline: \n",
      "    objective: reg:squarederror\n",
      "    min_child_weight: 4\n",
      "    colsample_bytree: .7\n",
      "    n_estimators: 200\n",
      "  tune:\n",
      "    cv: 5\n",
      "    objective: neg_mean_squared_error\n",
      "    search:\n",
      "      learning_rate: [.03, .05, .07,  .5]\n",
      "      max_depth: [3, 7, 9, 20]\n",
      "      alpha: [0, .5, 1]\n",
      "      subsample: [.6, .8, 1]\n",
      "data:\n",
      "  label: lnprice\n",
      "  test_size: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat ../src/carpricer.params.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a914ce",
   "metadata": {},
   "source": [
    "### Arguments to use\n",
    "\n",
    "Although our hyperparameters are stored in a file, the location of the file and the location of the data we are going to use are parameters we will need to pass to the notebook. Let's use the following arguments for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ac0f79",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "train_path = \"../data/car-prices/sample/automobile_prepared.csv\"\n",
    "params_file = \"../src/carpricer.params.yml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16234a60",
   "metadata": {},
   "source": [
    "> The cell above was market with a `tag` named `parameters` (To view it you have to show the tags toolbar in Jupyter Notebooks from the View menu). Cells marked with such tag are interpreted as parameters by `papermill` and hence you can run this notebook changing this parameters directly from the command line like this: `papermill notebooks/carpricer_model.ipynb --train_path data/car-prices/sample/automobile_prepared.csv --params_file src/carpricer.params.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19f08e",
   "metadata": {},
   "source": [
    "The library `jobtools` has functionalities to quickly parse a `YAML` file to a namespace object that can use the \"dot notation\" to access it's members. When running from the command line we don't need to load the file manually, but here since we are making an interactive run we can load it using `ParamsNamespace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a750bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobtools.arguments import ParamsNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afa4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamsNamespace.load(params_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e95f3b",
   "metadata": {},
   "source": [
    "## Loading data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce830e5",
   "metadata": {},
   "source": [
    "Let's use our preprocessing library to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2307d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = dataprep.read_and_split(train_path, \n",
    "                                                           params.data.test_size, \n",
    "                                                           params.data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ef72e",
   "metadata": {},
   "source": [
    "The method `scale_and_encode` was designed to preprocess a given dataset and also return the transformations fitted to it. This allows us to then store this transformations for when the model is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984f4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, transforms = transformations.scale_and_encode(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ac42f",
   "metadata": {},
   "source": [
    "Let's apply the same transformations to our testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d50f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = transforms.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22e3d1",
   "metadata": {},
   "source": [
    "## Training routine\n",
    "\n",
    "We will use Mlflow for tracking metrics, parameters and models in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a98ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d816c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f67c62",
   "metadata": {},
   "source": [
    "### The base model\n",
    "\n",
    "Our base model will be an XGBoost regressor with some basic configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c970ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df489d4c",
   "metadata": {},
   "source": [
    "We have some specific parameters for this regressor that are stored in the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c45d8cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(objective='reg:squarederror',\n",
       "          min_child_weight=4,\n",
       "          colsample_bytree=0.7,\n",
       "          n_estimators=200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.model.baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86ec86",
   "metadata": {},
   "source": [
    "We are going to pass this arguemnts to the constructor or the regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d6c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = XGBRegressor(silent=True, nthread=4, **params.model.baseline.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f24555",
   "metadata": {},
   "source": [
    "As a good practice, let's log the parameters used to our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_params(params.model.baseline.to_dict()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c14d44",
   "metadata": {},
   "source": [
    "### Searching for hyperparameters\n",
    "\n",
    "We designed a method to search for hyperparameters `fit_and_optimize`. This method recieves a training dataset, a base model and a grid with all the parameters to look for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cdb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = trainer.fit_and_optimize(X_train_transformed, \n",
    "                                  y_train, \n",
    "                                  base_model=base_model,\n",
    "                                  param_grid=params.model.tune.search.to_dict(),\n",
    "                                  cv=params.model.tune.cv,\n",
    "                                  scoring_fit=params.model.tune.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630cc69",
   "metadata": {},
   "source": [
    "### Evaluating the results\n",
    "\n",
    "Searching for the best configuration for the model may require some exploration. We are using the library `sklearn_evaluation` to evaluate results.\n",
    "\n",
    "`sklearn_evaluation` is used in the `evaluate_search` method where we pass the search results of the tunning and the parameters we are looking to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_search(search, plot_params_name=['learning_rate', 'max_depth'], to_mlflow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6893b",
   "metadata": {},
   "source": [
    "Let's fin the best model, the best parameters combination and log them with Mlflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = search.best_estimator_\n",
    "best_params = search.best_params_\n",
    "mlflow.log_params(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e451c9",
   "metadata": {},
   "source": [
    "We are going to calculate a couple of metrics for the chosen model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b7a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluator.evaluate_regressor(best_model, X_test_transformed, y_test)\n",
    "mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fbcb4f",
   "metadata": {},
   "source": [
    "### Constructing the inference pipeline\n",
    "\n",
    "Now we have a good model at hand, we can construct a pipeline with both the preprocessing step and an estimator. This pipleline can then be stored for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[('preprocessing', transforms),\n",
    "                                 ('model', best_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d0be0",
   "metadata": {},
   "source": [
    "#### Logging the model with Mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110bd03",
   "metadata": {},
   "source": [
    "We are going to log the winning model with mlflow. First let's see which is the expected inputs and outputs of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbacdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d22f2",
   "metadata": {},
   "source": [
    "Logging the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13987c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.log_model(model_pipeline, \"model\", signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c13691",
   "metadata": {},
   "source": [
    "Let's finish the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (carpricer)",
   "language": "python",
   "name": "carpricer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
